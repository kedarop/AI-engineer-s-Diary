{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5ebb93-1be2-467b-965e-729785063ea2",
   "metadata": {},
   "source": [
    "# NLP : Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50a6f3-cb9a-417d-9ac8-f96bfa5cab67",
   "metadata": {},
   "source": [
    "**1. Lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c052da0-4648-4c6a-9ad2-0aafcc53adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello, Nice to meet you!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609dcfb6-8be0-4062-b738-8517e1fbc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7049b53b-ddd6-4040-8861-f280667eb782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "print(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328140f4-c8a5-4674-87cd-0b60f5328212",
   "metadata": {},
   "source": [
    "**2. Remove Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ccfcaa9-6068-4b1a-8f77-51f0d5a9b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5951d4b6-29ed-4014-a007-2c4866d2007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1845588c-e7c9-46e3-ab1d-6b0a5723443c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abd96eba-1bc1-44b2-87f7-9bd307d34f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"It was too far to walk, and he did not want to go there by walking, So he took a cab and left an hour ago \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88474294-002a-4f37-9953-e5db9dd2bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentence = ' '.join([word for word in sentence.split() if word not in en_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fafb2623-a265-4dda-aa41-e813d185ab30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b91adeb8-1259-4e50-aeb7-5f2bcdb8bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Nice meet you!\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a03d75-ac19-45cd-adde-2ef10a5695f4",
   "metadata": {},
   "source": [
    "**3. Regular Expressions/ REGEX**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b5926-a148-4e2d-8493-29f6cd4ed869",
   "metadata": {},
   "source": [
    "Regular expressions, or regex for short, is a special syntax for searching for strings that meet a specified pattern. It is a great tool to filter and sort through text when you want to match patterns, rather than hard-coded string or strings. There are loads of options for the syntax, so it is best to just jump in and get started with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19493a4f-41b4-4aec-8ac9-d4dc85684aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5467f54-dff3-4550-937f-5764baa4c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\desktop\\notes\n"
     ]
    }
   ],
   "source": [
    "my_folder = r\"C:\\desktop\\notes\"\n",
    "print(my_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beca364a-b57b-4557-9d95-d55c7b83dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\desktop\n",
      "otes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\kedar\\AppData\\Local\\Temp\\ipykernel_7764\\4072164317.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  my_folder = \"C:\\desktop\\notes\"\n"
     ]
    }
   ],
   "source": [
    "my_folder = \"C:\\desktop\\notes\"\n",
    "print(my_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2698e-8973-4434-9cb6-c641bcf995dc",
   "metadata": {},
   "source": [
    "in the above example you can see the difference that after inserting \"r\" before the escape characters, the compiler ignores the escape sequences like \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d557f2fd-8ce6-4c8c-9adf-5dc34a173581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(22, 29), match='pattern'>\n"
     ]
    }
   ],
   "source": [
    "result_search = re.search(\"pattern\", r\"string to contain the pattern\")\n",
    "print(result_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c2796c-3dd6-4da9-80f7-52f803fdbaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "result_search2 = re.search(\"pattern\", r\"the phrase to find isn't in this string\")\n",
    "print(result_search2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "772ba4fe-927d-4e3e-8ff5-9b8e72c3ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = r\"dekault was able to crack the system\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "106e1943-d386-4371-a204-131e37fc932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = re.sub(\"dekault\", \"default\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9f633-d857-4956-a680-b0e807f0f306",
   "metadata": {},
   "source": [
    "we used the re.sub(wrong_word, correct_Word, name_of_vaeriable) to make corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "843823e8-853c-4a07-84be-dfcb2088d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default was able to crack the system\n"
     ]
    }
   ],
   "source": [
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e05a97-cac7-4e8d-8aef-199f84b6a4da",
   "metadata": {},
   "source": [
    "customer review example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98acd1e2-4bdd-4bc3-a4fa-b1d98b63422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_reviews=[\"josh was very rude to me\",\n",
    "                 \"emily helped me resolve the issue efficiently\",\n",
    "                 \"sarah was able to communicate well\",\n",
    "                 \"josh was helpful\",\n",
    "                 \"sarah was not professional during the meeting\",\n",
    "                 \"emily was efficient in her work\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0e407-f008-43af-a5e6-ff98dfa420ed",
   "metadata": {},
   "source": [
    "we need to find all the reviews associated with sarah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "099980ff-7d4d-4c63-8ede-f5c073388030",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarah_reviews=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6eeac595-cff5-4b7f-bbff-66a866d4b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_to_find = r\"sarah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72a0b39a-1c74-4548-a920-cf8406eca0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in customer_reviews:\n",
    "    if (re.search(pattern_to_find, string)):\n",
    "        sarah_reviews.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bb2f9b3-4ff7-4349-8f7a-4fb07be88097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sarah was able to communicate well', 'sarah was not professional during the meeting']\n"
     ]
    }
   ],
   "source": [
    "print(sarah_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79497b4d-601f-49fe-8a42-5879fd0b4bae",
   "metadata": {},
   "source": [
    "**4. TOKENIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88b05f-1360-4149-8b2c-496042ee6d22",
   "metadata": {},
   "source": [
    "A fundamental step in Natural Language Processing (NLP) involves converting our text into smaller units through a process known as tokenization. These smaller units are called tokens.\n",
    "\n",
    "Word tokenization is the most common form of tokenization, where individual words in the text become tokens. However, tokens can also be sentences, subwords, or individual characters, depending on your use case.\n",
    "\n",
    "We perform tokenization because the meaning of the overall text is better understood if we can analyze and understand the individual parts as well as the whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3641e9e-95e9-43ad-bd93-afe901da7ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8115fa97-59b0-45f5-abd4-afaa170572b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"Her dog's name is Max. Her cat's name is Luna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c719989-33b3-4eac-ab57-64dfc3dcd306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Her dog's name is Max.\", \"Her cat's name is Luna\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7d618ed-11da-4c8a-86c1-1fd46edf004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n ['Her', 'dog', \"'s\", 'name', 'is', 'Max', '.', 'Her', 'cat', \"'s\", 'name', 'is', 'Luna']\n"
     ]
    }
   ],
   "source": [
    "print('/n', word_tokenize(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6338a4-c4c8-4184-8d15-6fe18f2c15de",
   "metadata": {},
   "source": [
    "**5. STEMMING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c956677-3657-4d2b-9a6b-8bdcc7977c8b",
   "metadata": {},
   "source": [
    "Stemming reduces words to their base form by removing suffixes, aiding in text standardization.\n",
    "This process decreases the number of unique words, reducing dataset size and complexity.\n",
    "The Porter stemmer from the NLTK package is a common tool for stemming.\n",
    "Stemming can sometimes produce base forms that are not meaningful or proper words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f629a76-740f-40a3-93d6-b465ebe9c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d88e71a7-a953-4088-8138-925974ccd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "296d18fd-a50c-4985-884e-a04c632d3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_tokens = ('connecting', 'connected', 'connectivity', 'connection', 'connect', 'connects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23e4744b-4df8-44e9-b83e-59a25355ac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting :  connect\n",
      "connected :  connect\n",
      "connectivity :  connect\n",
      "connection :  connect\n",
      "connect :  connect\n",
      "connects :  connect\n"
     ]
    }
   ],
   "source": [
    "for t in connect_tokens:\n",
    "    print(t,\": \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d252ab31-d0b4-40ec-81e0-06f01b442b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_tokens = ('learning', 'learned', 'learns', 'learner', 'learners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "785daee7-8120-4609-a776-d6c337617505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning :  learn\n",
      "learned :  learn\n",
      "learns :  learn\n",
      "learner :  learner\n",
      "learners :  learner\n"
     ]
    }
   ],
   "source": [
    "for l in learn_tokens:\n",
    "    print(l,\": \", ps.stem(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36726ec-d749-43fa-a9a3-6306f466d27c",
   "metadata": {},
   "source": [
    "**5. LEMMATIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299ee98-d5fc-4612-9d8f-b8d0a2427bc2",
   "metadata": {},
   "source": [
    "Where stemming removes the last few characters of a word, lemmatization stems the word to a more meaningful base form and ensures it does not lose its meaning.\n",
    "\n",
    "Lemmatization works more intelligently by referencing a predefined dictionary containing the context of the word, and uses this when diminishing the word to its base form.\n",
    "\n",
    "This means we are often left with more meaningful words, but it does mean that we may be left with more words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a5af6ff-c3a3-4a10-945c-34d6c227dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23565fe7-bbd2-4cf2-bcb5-ba5a0df22f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
